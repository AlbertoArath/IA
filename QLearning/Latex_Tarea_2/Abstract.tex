\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}

\title{Q-Learning with Neural Networks for Robotic Arm Control}
\author{Pablo Sánchez Orozco, Alberto Arath Figueroa Salomon & Nicolás Every Alvarez }
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report explores the use of Q-learning with neural networks (NN) for training a robotic arm model in a physics-accurate simulation. The objective is to demonstrate the advantages of using NN-based Q-learning over traditional Q-tables, particularly in high-dimensional robotic control problems. The Mujoco-based model and the Menagie physics engine were employed for accurate physics simulation.
\end{abstract}

\section{Introduction}
The intention of this project was to train a robotic arm using Q-learning with neural networks (NN) instead of a conventional Q-table. The complexity of the robot's state-action space necessitates a function approximator, which NN provides efficiently. 

\section{Methodology}
The robotic arm was simulated using Mujoco, with Menagie serving as the physics engine. The model consists of multiple articulated joints, each actuated by motors. The XML model features:
\begin{itemize}
    \item Six degrees of freedom: Base rotation, shoulder pitch, elbow movement, wrist pitch and roll, and jaw control.
    \item Defined friction loss and armature parameters for realistic motion.
    \item Motor constraints with position-based control.
    \item Contact modeling to simulate object interactions.
\end{itemize}

The Q-learning algorithm with NN was used to approximate the Q-value function, enabling generalization over large state-action spaces.

\section{Mathematical Formulation}
The Q-learning update rule is given by:
\begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\end{equation}
where:
\begin{itemize}
    \item $Q(s, a)$ is the Q-value for state $s$ and action $a$.
    \item $\alpha$ is the learning rate.
    \item $\gamma$ is the discount factor.
    \item $r$ is the received reward.
    \item $s'$ is the next state.
\end{itemize}
Instead of storing $Q(s,a)$ in a table, we approximate it using a neural network:
\begin{equation}
    Q(s, a; \theta) \approx f(s, a; \theta)
\end{equation}
where $\theta$ represents the NN parameters trained via backpropagation.

\section{Neural Network Design}
The neural network used for Q-learning is designed as follows:
\begin{itemize}
    \item \textbf{Input}: The state vector containing the current angles of all joints.
    \item \textbf{Output}: Q-values corresponding to the two possible actions (increase or decrease angle) for each joint.
    \item \textbf{Size}: If the robotic arm has $n$ joints, the output layer consists of $2n$ neurons.
\end{itemize}
The reward mechanism is defined based on the Euclidean distance between the tip of the claw and a target point in space. The objective is to minimize this distance, guiding the robotic arm towards the target.

\section{Why Use NN Instead of Q-table?}
A traditional Q-table is infeasible for a high-dimensional state space as it requires an entry for each state-action pair. Given the continuous action space of the robotic arm, NN provides a better alternative by enabling function approximation and generalization, reducing memory requirements and training time.

\section{Results and Discussion}
While the implementation of Q-learning with neural networks showed potential, the model requires further refinement. One of the primary issues was the selection mechanism using the $\arg\max$ function. Instead of selecting the best Q-value for each joint separately, it was only choosing a single action across the entire output vector. Since each joint has two possible actions, the correct approach should involve selecting the best index within each joint's respective Q-value pair. This misalignment caused suboptimal policy execution, leading to ineffective control of the robotic arm. Future improvements should focus on correctly implementing $\arg\max$ across joint-specific action pairs to ensure proper action selection per joint.
Additionally, hyperparameter tuning using a grid search approach should be explored to optimize learning efficiency and convergence.

\section{References}
Watkins, C. J., \& Dayan, P. (1992). Q-learning. \textit{Machine Learning}, \textbf{8}(3-4), 279-292. 

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... \& Hassabis, D. (2015). Human-level control through deep reinforcement learning. \textit{Nature}, \textbf{518}(7540), 529-533.

Mujoco Physics Engine: \url{https://mujoco.org/}

Mendeliere Repository: \url{https://github.com/mendeliere}


\end{document}
